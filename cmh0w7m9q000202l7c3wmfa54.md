---
title: "Como Implementar IA no Azure Utilizando Terraform e Ferramentas de Observabilidade"
seoTitle: "Implementando IA no Azure com Terraform"
seoDescription: "Implement AI on Azure with Terraform, focusing on MLOps, governance, and secure architecture using IaC principles and observability tools"
datePublished: Tue Oct 21 2025 18:23:32 GMT+0000 (Coordinated Universal Time)
cuid: cmh0w7m9q000202l7c3wmfa54
slug: como-implementar-ia-no-azure-utilizando-terraform-e-ferramentas-de-observabilidade
cover: https://cdn.hashnode.com/res/hashnode/image/upload/v1761070839630/f85da92e-5788-4ff9-8d8d-ab2c91ce9393.png
ogImage: https://cdn.hashnode.com/res/hashnode/image/upload/v1761070980984/eab441c3-067d-4688-81be-a6217f1a9e5a.png
tags: ai, azure, devops, terraform, azure-certified, mlops

---

## **I. Imperativos Estratégicos: IaC, MLOps e a Fundação Azure**

### **Introdução à Arquitetura MLOps e o Papel do Terraform**

A implementação bem-sucedida de aplicações de Inteligência Artificial (IA) em ambientes de produção de larga escala exige a adoção de princípios de Machine Learning Operations (MLOps). Central a esta abordagem é a Infraestrutura como Código (IaC), onde o Terraform se estabelece como a ferramenta ideal para definir, provisionar e gerenciar a infraestrutura do Azure de forma repetível e previsível.1 Ao utilizar arquivos de configuração declarativos, o Terraform garante que a infraestrutura subjacente (rede, computação, armazenamento e observabilidade) seja tratada como código, permitindo que a equipe de engenharia trate a arquitetura do ambiente de ML como um recurso versionável e auditável.

A *stack* tecnológica recomendada para uma aplicação de IA moderna no Azure tipicamente inclui o Azure Machine Learning (AML) como plataforma central de IA, o Azure Container Apps (ACA) para o serviço de inferência *serverless*, e mecanismos de ingestão de dados como Azure Data Factory (ADF) ou Azure Event Hubs. A unificação da gestão desses componentes críticos via Terraform é a espinha dorsal desta arquitetura MLOps.

### **Princípios de Governança e Gerenciamento de Estado Remoto**

O arquivo de estado do Terraform (tfstate) é um componente altamente sensível, pois contém o mapeamento de todos os recursos provisionados e, potencialmente, dados confidenciais (como IDs de recursos e FQDNs). Por essa razão, ele jamais deve ser armazenado localmente. O gerenciamento seguro exige o uso do *backend* azurerm, que armazena o estado como um *Blob* em um Azure Storage Account, garantindo bloqueio de estado e verificação de consistência.2

Para produção, a autenticação ao *data plane* da conta de armazenamento deve ser rigorosamente controlada. O método recomendado é o uso de **Azure Active Directory (AAD)** ou **Managed Identity (MI)**, afastando-se do uso de Access Keys estáticas ou Tokens SAS. A dependência de chaves estáticas representa uma vulnerabilidade significativa, pois essas chaves, se comprometidas, podem expor o estado do Terraform a ataques. Ao configurar o *backend* para autenticação via MI (especialmente com OpenID Connect/Workload Identity Federation), o acesso é delegado e gerenciado pela Azure, concedendo credenciais temporárias ao *runner* de CI/CD, minimizando drasticamente a superfície de ataque e aderindo ao Princípio do Privilégio Mínimo (PoLP).

### **Estrutura Modular e Reutilização (MLOps Component-Based)**

Para suportar múltiplos ambientes (como desenvolvimento, teste e produção) e promover a reutilização, o projeto Terraform deve seguir uma estrutura modular clara. As diretrizes de engenharia de software recomendam a separação lógica em módulos reutilizáveis (modules/) e configurações de ambiente raiz (environments/dev, environments/prod).3

Cada componente lógico da infraestrutura deve ser encapsulado em seu próprio módulo. Por exemplo, um módulo ml\_platform pode ser criado para provisionar o Azure ML Workspace, o Application Insights e as contas de armazenamento associadas.3 O diretório raiz de cada ambiente (environments/prod) então invoca este módulo, passando apenas as variáveis específicas daquele ambiente (ex: SKUs, nomes de recursos, restrições de rede).3

Dentro de cada módulo, a consistência é vital. Recomenda-se o uso de arquivos como main.tf, variables.tf, e outputs.tf. Variáveis e saídas devem incluir descrições detalhadas, e as saídas, como storage\_primary\_connection\_string, devem seguir um padrão descritivo ({nome}\_{atributo}) para serem compreensíveis fora do escopo do módulo.3

O sucesso da arquitetura IaC depende da clareza na separação de responsabilidades, conforme detalhado na estrutura proposta:

`Tabela 1: Estrutura Modular Recomendada do Terraform para MLOps`

<table><tbody><tr><td colspan="1" rowspan="1"><p><strong>Diretório Raiz</strong></p></td><td colspan="1" rowspan="1"><p><strong>Descrição</strong></p></td><td colspan="1" rowspan="1"><p><strong>Recursos/Finalidade</strong></p></td></tr><tr><td colspan="1" rowspan="1"><p>modules/network</p></td><td colspan="1" rowspan="1"><p>Módulo VNet, Subnets, Private DNS Zones</p></td><td colspan="1" rowspan="1"><p>Perímetro de rede segura.</p></td></tr><tr><td colspan="1" rowspan="1"><p>modules/ml_platform</p></td><td colspan="1" rowspan="1"><p>Módulo central para a plataforma de IA.</p></td><td colspan="1" rowspan="1"><p>Azure ML Workspace (AVM), Key Vault, Application Insights.</p></td></tr><tr><td colspan="1" rowspan="1"><p>modules/ingestion_adf</p></td><td colspan="1" rowspan="1"><p>Módulo para pipeline de dados batch.</p></td><td colspan="1" rowspan="1"><p>Azure Data Factory, Linked Services, Datasets, Pipelines.</p></td></tr><tr><td colspan="1" rowspan="1"><p>environments/prod</p></td><td colspan="1" rowspan="1"><p>Configuração raiz para ambiente de Produção.</p></td><td colspan="1" rowspan="1"><p>Define variáveis, chama módulos e configura o backend MI/AAD.</p></td></tr></tbody></table>

## **II. Governança e Conformidade via Policy as Code (PaC)**

**Enforcing de Tagging para Gerenciamento de Custos**

A gestão eficaz de custos e a visibilidade de recursos no Azure dependem de uma estratégia de *tagging* robusta. As tags são pares chave-valor (ex: env:production ou cost-center:project-ml) que permitem identificar e alocar despesas com precisão.

Para garantir que o tagging seja aplicado de forma universal e consistente, deve-se implementar o conceito de Policy as Code (PaC) utilizando o recurso azurerm\_policy\_definition do Terraform.10 Definir políticas personalizadas que obrigam a presença de tags críticas em todos os recursos (Resource Groups, ML Workspaces, Container Apps) é uma prática fundamental de governança.

Em ambientes de produção, o efeito da política deve ser configurado como Deny, garantindo que qualquer tentativa de provisionar um recurso sem as tags obrigatórias seja negada pelo Azure, impondo a conformidade desde o momento da implantação.

  
**Otimização de Custos e Capacidade (IaC)**

A otimização de custos deve ser um fator primário na definição da infraestrutura via IaC. Isso inclui o dimensionamento correto dos serviços (*right-sizing*) e a utilização de recursos de escalabilidade nativa.6 Para a API de inferência, o Azure Container Apps (ACA) gerencia o *autoscaling* de forma intrínseca através da sua definição de *template*, permitindo que o número de réplicas se ajuste dinamicamente à demanda.12

Para *workloads* de treinamento ou inferência de longa duração com requisitos de máquinas virtuais pesadas (como clusters de GPU no Azure ML), o uso de Reserved Instances (RIs) ou Capacity Reservations oferece descontos substanciais.

O provisionamento do recurso azurerm\_capacity\_reservation via Terraform é um mecanismo poderoso para otimizar os gastos. Ao integrar a reserva de capacidade diretamente no módulo de computação de ML, a equipe de engenharia assegura que a infraestrutura provisionada para treinamento pesado (por exemplo, famílias de VM Standard\_ND...) já esteja coberta pela taxa de desconto contratada.

O desconto é aplicado automaticamente se a VM provisionada pelo Compute Cluster corresponder à reserva, garantindo que a otimização financeira faça parte do *deployment* atômico da infraestrutura.

## **III. Fundação de Rede Segura para Workloads de IA (Zero Trust)**  

## **Design da VNet e Sub-Redes**

A segurança de uma aplicação de IA em produção começa com a rede. O Terraform deve ser utilizado para definir centralmente a Virtual Network (VNet) e suas sub-redes constituintes. É essencial alocar sub-redes dedicadas para o Azure ML Workspace, o Azure Container Apps Environment e, separadamente, para os Private Endpoints que se conectarão aos serviços de dados.  

### **Implementação de Private Link e Private DNS Zones**

Para aderir ao princípio Zero Trust, todos os serviços críticos de IA e dados devem ser acessados exclusivamente pela rede privada. O Terraform provisiona Private Endpoints (azurerm\_private\_endpoint) para proteger recursos como o Azure ML Workspace, Azure Storage Accounts (artefatos e dados), Azure Key Vault e os serviços de ingestão de dados (ADF/Event Hubs).

A resolução de nomes dentro da VNet requer a configuração de Private DNS Zones. O Terraform deve criar as zonas apropriadas (ex: privatelink.azureml.net, privatelink.blob.core.windows.net) e associá-las à VNet através do recurso azurerm\_private\_dns\_zone\_virtual\_network\_link.  

### **Gestão de Identidade e Acesso (RBAC & Managed Identity)**  

A segurança de rede (Private Endpoint) é um perímetro defensivo, mas deve ser complementada com o controle rigoroso de identidade. As Managed Identities (MI) são o método preferencial para comunicação segura *service-to-service* sem a necessidade de gerenciar credenciais.

O Terraform é o responsável por definir explicitamente as atribuições de função (Role-Based Access Control - RBAC) necessárias para que as MIs operacionais acessem outros recursos. O recurso azurerm\_role\_assignment permite conceder permissões específicas.

Por exemplo, a Managed Identity do Azure ML Compute Cluster necessita de permissão de *Storage Blob Data Contributor* na conta de armazenamento de artefatos para carregar dados de treinamento.

O provisionamento seguro exige que a **identidade** e a **rede** trabalhem em conjunto. A simples criação de um Private Endpoint (acesso de rede) sem garantir que o serviço consumidor utilize uma MI com RBAC adequado representa uma falha de segurança latente. Ao forçar o uso de MI (como use\_managed\_identity = true em um Linked Service do ADF 20) e restringir o acesso apenas pela rede privada, o risco de exfiltração de chaves ou acesso não autorizado é mitigado. A arquitetura IaC deve impor essa política combinada.

## **IV. Provisionamento da Plataforma de IA (Azure Machine Learning)**  

### **Azure ML Workspace / AI Hub via AVM**

O Azure Machine Learning Workspace é o recurso central para gerenciar o ciclo de vida do ML. Para garantir aderência às melhores práticas da Microsoft, é altamente recomendada a utilização do Azure Verified Module (AVM) Azure/avm-res-machinelearningservices-workspace. Este módulo provisiona o Workspace junto com suas dependências essenciais (Key Vault, Storage Account e Application Insights).

O parâmetro kind dentro deste módulo reflete a evolução da plataforma Azure. Ele permite provisionar um Workspace padrão (Default), um **AI Hub** (que oferece uma experiência aprimorada para casos de uso de IA Generativa) ou um **AI Project**.4 Para novas aplicações GenAI, a definição de um Hub é a escolha estratégica. O módulo deve ser configurado para desativar o acesso público e integrar o Private Endpoint previamente definido no módulo de rede, garantindo que a plataforma de IA esteja em conformidade com o perímetro de segurança.

### **Compute Clusters para o Ciclo de Vida do ML**

O ciclo de vida do ML, incluindo treinamento e processamento de dados, requer clusters de computação dedicados. Estes são provisionados utilizando recursos como o azurerm\_machine\_learning\_compute\_cluster.

É fundamental que o Terraform não apenas crie o cluster, mas também configure sua identidade gerenciada. A MI associada ao Compute Cluster deve receber explicitamente as atribuições de função necessárias (azurerm\_role\_assignment) para ler e gravar dados nos Azure Storage Accounts protegidos por Private Endpoint.

## **V. Ingestão de Dados na Fonte com Terraform (Data Ingestion as Code)**

O IaC deve se estender à ingestão de dados, garantindo que o acesso à fonte de dados seja provisionado com a mesma segurança e repetibilidade que o restante da infraestrutura de IA.

### **Pipeline de Dados Batch com Azure Data Factory (ADF)**

Para cenários de processamento em lote, o Azure Data Factory (ADF) é a ferramenta de orquestração padrão. O Terraform deve provisionar o recurso azurerm\_data\_factory.

O aspecto mais importante para a segurança é a configuração dos *Linked Services*, que definem a conexão com fontes e destinos (ex: Azure SQL Database, Storage). O recurso azurerm\_data\_factory\_linked\_service\_... deve ser configurado com a autenticação Managed Identity (use\_managed\_identity = true), eliminando a necessidade de gerenciar connection\_string sensíveis no ADF ou no código Terraform. O Terraform também provisiona os *pipelines* de orquestração (azurerm\_data\_factory\_pipeline) e os datasets associados.

### **Ingestão de Streaming com Azure Event Hubs**

Para dados de *streaming* em tempo real, o Azure Event Hubs é a solução de escolha. O Terraform é utilizado para provisionar o *Namespace* e as instâncias de *Event Hub*.

Similar ao ADF, o Namespace do Event Hubs deve ser integrado à VNet usando Private Link se os produtores de dados estiverem em um ambiente privado, e as políticas de acesso (como SAS policies) devem ser definidas com o princípio de mínimo privilégio.

É uma exigência arquitetural que o pipeline de ingestão mantenha a consistência da segurança de rede com a plataforma de IA. Se o Azure ML Workspace acessa o *Storage Account* via Private Endpoint, o pipeline de ingestão (ADF ou Event Hubs) também deve usar Managed Identity e Private Endpoint para acessar o mesmo *Storage Account*, garantindo que todo o caminho dos dados (Source -&gt; Ingestion Service -&gt; Storage -&gt; Azure ML) seja restrito à rede privada.

## **VI. Deployment de Inferência em Produção com Azure Container Apps (ACA)**

### **Justificativa Técnica para ACA**

O Azure Container Apps (ACA) é a plataforma *serverless* recomendada para hospedar a API de inferência do modelo de IA. Sua arquitetura simplifica o *deployment* e a escalabilidade de microsserviços e aplicações orientadas por IA, oferecendo suporte nativo a contêineres e cargas de trabalho otimizadas, incluindo a capacidade de solicitar recursos de GPU em ambientes configurados. O Terraform gerencia o ACA App (azurerm\_container\_app) e o Environment subjacente.

### **Provisionamento do ACA e Configuração de Autoscaling**

O *deployment* de inferência começa com o provisionamento do ACA Environment, que deve ser integrado à VNet para comunicação segura.

O recurso principal, azurerm\_container\_app, define a imagem do contêiner, o modo de revisão (revision\_mode) e, fundamentalmente, as regras de *autoscaling* dentro do bloco template.

O ACA oferece *autoscaling* baseado em HTTP ou outros *triggers* personalizados, garantindo que o consumo de recursos seja otimizado de acordo com a demanda real.

### **Injeção Segura de Variáveis de Ambiente e Segredos**

A segurança em tempo de execução da aplicação de IA requer que credenciais sensíveis sejam injetadas como segredos, não como texto simples. O recurso azurerm\_container\_app permite definir blocos secret que armazenam informações confidenciais (ex: *tokens* de acesso ou chaves de API). Essas informações são então referenciadas de forma segura no bloco env do contêiner.

Este mecanismo é vital para a observabilidade. A *connection string* do Application Insights, que é necessária para enviar dados de telemetria, deve ser tratada como um segredo injetado no contêiner de inferência através da variável de ambiente padrão APPLICATIONINSIGHTS\_CONNECTION\_STRING.

## **VII. Implementando Observabilidade End-to-End com OpenTelemetry**

### **Provisionamento e Conexão do Application Insights**

A observabilidade completa (logs, métricas e traces distribuídos) é crucial para monitorar a saúde e o desempenho do modelo em produção. O Terraform provisiona o *sink* de telemetria utilizando o recurso azurerm\_application\_insights. Recomenda-se que este recurso seja provisionado como parte do módulo ml\_platform para garantir sua criação com as demais dependências de IA.

A *connection string* gerada pelo Application Insights é um atributo essencial que deve ser exposto como uma saída (output) do módulo. Esta saída é então consumida pelo módulo de *deployment* do ACA para configurar a injeção segura de variáveis de ambiente, estabelecendo a ponte entre a aplicação e o backend de monitoramento.

### **Estratégia de Instrumentação de Código-Fonte (OpenTelemetry)**

A estratégia de instrumentação deve focar no OpenTelemetry (Otel) como o padrão *vendor-agnostic* para coleta de telemetria. Para aplicações em linguagens suportadas como Python,.NET ou Java, a **Azure Monitor OpenTelemetry Distro** é o método preferido. Esta distribuição automatiza a instalação dos *exporters* e das bibliotecas de instrumentação necessárias para coletar traces, métricas e logs.

A configuração no código-fonte é minimalista. Por exemplo, em Python, a função configure\_azure\_monitor() é chamada no *startup* da aplicação, utilizando a *connection string* fornecida pelo ambiente de execução.

### **Conexão IaC e Contexto de Telemetria**

O Terraform garante que a infraestrutura de observabilidade e a aplicação de inferência estejam perfeitamente sincronizadas. Ao injetar a *connection string* como uma variável de ambiente, o IaC abstrai a lógica de conexão do código do modelo de IA. Isso desvincula a observabilidade da aplicação, permitindo que o código do modelo seja instrumentado uma única vez com o padrão Otel, enquanto o Terraform decide o destino da telemetria.

Além da *connection string*, o Terraform pode injetar variáveis de ambiente para definir o contexto da telemetria, especificamente o Cloud Role Name e Cloud Role Instance.31 Definir o Cloud Role Name com um valor descritivo (e.g., ML-Inference-API-Prod) é crucial para que o Application Map do Azure Monitor visualize o Container App de inferência como um nó lógico distinto na arquitetura, facilitando o diagnóstico de latência e erros.

`Tabela 2: Matriz de Componentes de Runtime e Observabilidade (Otel)`

<table><tbody><tr><td colspan="1" rowspan="1"><p><strong>Componente (Deployment)</strong></p></td><td colspan="1" rowspan="1"><p><strong>Recurso Azure</strong></p></td><td colspan="1" rowspan="1"><p><strong>Tipo de Telemetria (OpenTelemetry)</strong></p></td><td colspan="1" rowspan="1"><p><strong>Configuração via Terraform</strong></p></td></tr><tr><td colspan="1" rowspan="1"><p>API de Inferência</p></td><td colspan="1" rowspan="1"><p>Azure Container Apps</p></td><td colspan="1" rowspan="1"><p>Traces (Latência), Métricas (RPS, Erro), Logs</p></td><td colspan="1" rowspan="1"><p>Injeção do Connection String via secret na variável APPLICATIONINSIGHTS_CONNECTION_STRING</p></td></tr><tr><td colspan="1" rowspan="1"><p>Azure ML Training</p></td><td colspan="1" rowspan="1"><p>Azure ML Compute Cluster</p></td><td colspan="1" rowspan="1"><p>Métricas de Experimento, Logs de Treinamento</p></td><td colspan="1" rowspan="1"><p>Application Insights provisionado no módulo ML Platform.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Data Ingestion</p></td><td colspan="1" rowspan="1"><p>Azure Data Factory / Event Hubs</p></td><td colspan="1" rowspan="1"><p>Logs de Diagnóstico, Métricas de Vazão</p></td><td colspan="1" rowspan="1"><p>Configuração de Log Analytics/App Insights via Private Link (se aplicável).</p></td></tr></tbody></table>

A definição do *sink* de telemetria via azurerm\_application\_insights e sua injeção no ACA via bloco secret cria uma dependência estrita, garantindo que a aplicação esteja sempre instrumentada e enviando dados para o destino correto. Se o backend de observabilidade precisar ser alterado, o Terraform é o único ponto de alteração, mantendo a aplicação de IA isolada das mudanças na infraestrutura de monitoramento.

---

## **VIII. Síntese e Conclusão: O Ciclo MLOps Reforçado pelo IaC**

O *deployment* de uma aplicação de IA no Azure utilizando Terraform transcende a simples automação de infraestrutura. Ele estabelece uma arquitetura robusta de MLOps que impõe governança, segurança e otimização de custos desde o design.

### **Validação do Ciclo de Vida MLOps com Terraform**

O uso de uma estrutura modular (separando modules/ e environments/) garante que as configurações de produção e desenvolvimento sejam uniformes em termos de componentes, diferindo apenas nas especificações de variáveis (SKUs, redes).

O Terraform atua como o motor que valida e aplica a arquitetura MLOps, desde a ingestão de dados até o *deployment* da API de inferência.

### **Auditoria de Segurança e Conformidade (Checklist IaC)**

A segurança em ambientes de IA de produção é alcançada através da convergência de múltiplos controles, todos declarados e auditáveis via Terraform. Os requisitos de segurança para um *deployment* de alto nível incluem:

1. **Gerenciamento de Estado:** O acesso ao tfstate deve ser autenticado exclusivamente por Azure Active Directory ou Managed Identity, eliminando Access Keys estáticas.
    
2. **Perímetro de Rede:** Todos os serviços críticos (ML Workspace, Storage, Key Vault, ADF/Event Hubs) devem ser protegidos por Private Endpoints, isolando o tráfego dentro da VNet.
    
3. **Controle de Identidade:** A comunicação *service-to-service* deve utilizar Managed Identity, com permissões estritamente definidas por azurerm\_role\_assignment (PoLP).
    
4. **Ingestão de Dados:** Os Linked Services do Azure Data Factory devem forçar a autenticação via Managed Identity (use\_managed\_identity = true).
    
5. **Governança:** A conformidade de *tagging* (para rastreamento de custos e alocação) deve ser imposta via Azure Policy (azurerm\_policy\_definition) com efeito Deny.
    
      
    

---

### **Próximos Passos e Expansão da Arquitetura**

Para expandir a maturidade do MLOps, o IaC deve ser estendido para gerenciar recursos além da infraestrutura pura. Isso inclui a gestão de *pipelines* de treinamento dentro do próprio Azure ML (utilizando, por exemplo, *pipeline* v2) e a automatização da gestão do Registro de Modelos.

A otimização contínua de custos deve monitorar o uso de capacidade computacional e integrar decisões de redimensionamento de recursos (como *compute clusters*) diretamente nas variáveis do Terraform, alinhando despesas com os requisitos de desempenho do modelo.

---